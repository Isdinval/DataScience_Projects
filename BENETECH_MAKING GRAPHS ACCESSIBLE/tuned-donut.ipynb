{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Donut with additional hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"import re\nfrom pathlib import Path\nfrom typing import List\nfrom functools import partial\n\nfrom transformers import (\n    DonutProcessor,\n    VisionEncoderDecoderConfig,\n    VisionEncoderDecoderModel,\n)\nimport torch\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\nfrom datasets import Image as ds_img\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    test_grayscale = True\n    debug_clean = False\n    batch_size = 4\n    image_path = \"/kaggle/input/benetech-making-graphs-accessible/test/images\"\n    max_length = 512\n    model_dir = \"/kaggle/input/benetech-donut\"\n\n    \nBOS_TOKEN = \"<|BOS|>\"\nX_START = \"<x_start>\"\nX_END = \"<x_end>\"\nY_START = \"<y_start>\"\nY_END = \"<y_end>\"\n\nPLACEHOLDER_DATA_SERIES = \"0;0\"\nPLACEHOLDER_CHART_TYPE = \"line\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List, Tuple\ndef clean_preds(x: List[str], y: List[str]) -> Tuple[List[str], List[str]]:\n    def clean(value):\n        value = re.sub(r\"[^\\d.\\-eE]\", \"\", value)\n        if value.count(\".\") > 1:\n            value = value.replace(\".\", \"\", 1)\n        if value.count(\"-\") > 1:\n            value = value.replace(\"-\", \"\")\n        if value.count(\"e\") > 1:\n            value = value.replace(\"e\", \"\", 1)\n        return value\n\n    def clean_list(str_list):\n        cleaned_list = [clean(val) for val in str_list if val.strip()]\n        return cleaned_list\n\n    x_cleaned = clean_list(x)\n    y_cleaned = clean_list(y)\n\n    return x_cleaned, y_cleaned\n    \n\ndef string2preds(pred_string: str) -> Tuple[str, List[str], List[str]]:\n    chart_type_mapping = {\n        \"<dot>\": \"dot\",\n        \"<horizontal_bar>\": \"horizontal_bar\",\n        \"<vertical_bar>\": \"vertical_bar\",\n        \"<scatter>\": \"scatter\",\n        \"<line>\": \"line\",\n    }\n\n    for token, chart_type in chart_type_mapping.items():\n        if token in pred_string:\n            break\n    else:\n        return \"vertical_bar\", [], []\n\n    pattern = r\"{}{}(.*?){}{}(.*?){}\".format(\n        X_START, X_END, Y_START, Y_END, chart_type_mapping[token]\n    )\n    match = re.search(pattern, pred_string)\n    if not match:\n        return chart_type, [], []\n\n    x_values = match.group(1).split(\";\")\n    y_values = match.group(2).split(\";\")\n\n    x_cleaned, y_cleaned = clean_preds(x_values, y_values)\n\n    return chart_type, x_cleaned, y_cleaned","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_dir = Path(CFG.image_path)\nimages = list(image_dir.glob(\"*.jpg\"))\n\nds = Dataset.from_dict(\n    {\"image_path\": [str(x) for x in images], \"id\": [x.stem for x in images]}\n).cast_column(\"image_path\", ds_img())\n\ndef preprocess(examples, processor):\n    pixel_values = []\n\n    for sample in examples[\"image_path\"]:\n        arr = np.array(sample)\n        \n        # There are some grayscale images that were making this fail\n        # This prevents that.\n        if len(arr.shape) == 2:\n            print(\"Changing grayscale to 3 channel format\")\n            print(arr.shape)\n            arr = np.stack([arr]*3, axis=-1)\n        \n        pixel_values.append(processor(arr, random_padding=True).pixel_values)\n        \n        \n    return {\n        \"pixel_values\": torch.tensor(np.vstack(pixel_values)),\n    }\n\nmodel = VisionEncoderDecoderModel.from_pretrained(CFG.model_dir)\nmodel.eval()\n\ndevice = torch.device(\"cuda:0\")\n\nmodel.to(device)\ndecoder_start_token_id = model.config.decoder_start_token_id\nprocessor = DonutProcessor.from_pretrained(CFG.model_dir)\n\nids = ds[\"id\"]\nds.set_transform(partial(preprocess, processor=processor))\n\ndata_loader = DataLoader(\n    ds, batch_size=CFG.batch_size, shuffle=False\n)\n\n\nfrom tqdm import tqdm\n\nall_generations = []\nwith tqdm(total=len(data_loader)) as progress_bar:\n    for batch in data_loader:\n        pixel_values = batch[\"pixel_values\"].to(device)\n        batch_size = pixel_values.shape[0]\n        decoder_input_ids = torch.full(\n            (batch_size, 1),\n            decoder_start_token_id,\n            device=pixel_values.device,\n        )\n\n        outputs = model.generate(\n            pixel_values,\n            decoder_input_ids=decoder_input_ids,\n            max_length=CFG.max_length,\n            early_stopping=True,\n            pad_token_id=processor.tokenizer.pad_token_id,\n            eos_token_id=processor.tokenizer.eos_token_id,\n            use_cache=True,\n            num_beams=2,\n            temperature=.9,\n            top_k=1,\n            top_p=.4,\n            return_dict_in_generate=True,\n        )\n\n        all_generations.extend(processor.batch_decode(outputs.sequences))\n        progress_bar.update(1)\n\nchart_types, x_preds, y_preds = [], [], []\nfor gen in all_generations:\n    try:\n        chart_type, x, y = string2preds(gen)\n        new_chart_type = chart_type\n        x_str = \";\".join(map(str, x))\n        y_str = \";\".join(map(str, y))\n    except Exception as e:\n        print(\"Failed to convert to string:\", gen)\n        print(e)\n        new_chart_type = PLACEHOLDER_CHART_TYPE\n        x_str = PLACEHOLDER_DATA_SERIES\n        y_str = PLACEHOLDER_DATA_SERIES\n\n    if len(x_str) == 0:\n        x_str = PLACEHOLDER_DATA_SERIES\n    if len(y_str) == 0:\n        y_str = PLACEHOLDER_DATA_SERIES\n\n    chart_types.append(new_chart_type)\n    x_preds.append(x_str)\n    y_preds.append(y_str)\n    \n        \n\nsub_df = pd.DataFrame(\n    data={\n        \"id\": [f\"{id_}_x\" for id_ in ids] + [f\"{id_}_y\" for id_ in ids],\n        \"data_series\": x_preds + y_preds,\n        \"chart_type\": chart_types * 2,\n    }\n)\n\nsub_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(sub_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = VisionEncoderDecoderModel.from_pretrained(CFG.model_dir)\n\nmodel.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}